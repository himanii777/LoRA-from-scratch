{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====Check CUDA Device Availability===#\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "#===Run this if multiple GPUs are available===#\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "\n",
    "\n",
    "# import torch\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Device:', device)\n",
    "# print('Current cuda device: ', torch.cuda.current_device())\n",
    "# print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===Import Libraries===#\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments, RobertaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff652f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===Set Seed for Reproducibility===#\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe8151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======LoRA Class======#\n",
    "\n",
    "\"\"\"\n",
    "Here we are going to create a LORA linear class and since we aren't modifying all the layers in roberta, we later extract the exact layer we want to modify with its own ran via apply_lora_by_names.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LoraLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=0, lora_alpha=1.0, lora_dropout=0.1, bias=True, freeze_base=True):\n",
    "        \"\"\"\n",
    "        We will  be training weight matrices via nn.Linear. In and out features have the same size as the original layer.\n",
    "        LoRA parameters A and B are initialized to zero. The scaling factor is set to lora_alpha/r.\n",
    "        r, and lora_alpha will be customized to each layer from apply_lora_by_names.\n",
    "        The LoRA dropout is applied to the input of the A matrix.\n",
    "        we aren't modifying the original weight so freeze_base is set to True.\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_alpha = 2*r\n",
    "        self.scaling = lora_alpha / r if r > 0 and r != 0 else 1  \n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.zeros(r, in_features))       # A: (r x in_features)\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_features, r))      # B: (out_features x r)\n",
    "            nn.init.kaiming_uniform_(self.lora_B, a=math.sqrt(5))         \n",
    "        else:\n",
    "            # If r=0, LoRA is disabled (just a frozen linear layer)\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "\n",
    "        if freeze_base:\n",
    "            self.weight.requires_grad = False\n",
    "            if self.bias is not None:\n",
    "                self.bias.requires_grad = False\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "        if self.r > 0:\n",
    "            lora_out = torch.matmul(self.lora_dropout(x), self.lora_A.T)\n",
    "            lora_out = torch.matmul(lora_out, self.lora_B.T)\n",
    "            result = result + lora_out * self.scaling\n",
    "        return result\n",
    "\n",
    "\n",
    "def apply_lora_by_names(model, rank_dict, freeze=True, task_type=\"seq_cls\"):\n",
    "    \"\"\"\n",
    "    Replace Linear layers in the model with LoraLinear according to rank_dict.\n",
    "    Supports: seq_cls , causal_lm , seq2seq.\n",
    "    \"\"\"\n",
    "\n",
    "    for param_name, r in rank_dict.items():\n",
    "        if not param_name.endswith(\".weight\"):\n",
    "            continue\n",
    "        module_name = param_name[:-len(\".weight\")]\n",
    "        parent_name, child_name = module_name.rsplit(\".\", 1)\n",
    "        parent_module = model.get_submodule(parent_name)\n",
    "        if not hasattr(parent_module, child_name):\n",
    "            continue\n",
    "        old_linear = getattr(parent_module, child_name)\n",
    "        if not isinstance(old_linear, nn.Linear):\n",
    "            continue\n",
    "\n",
    "        if r == 0:\n",
    "            if freeze:\n",
    "                old_linear.weight.requires_grad = False\n",
    "                if old_linear.bias is not None:\n",
    "                    old_linear.bias.requires_grad = False\n",
    "            continue\n",
    "\n",
    "        new_linear = LoraLinear(\n",
    "            old_linear.in_features,\n",
    "            old_linear.out_features,\n",
    "            r=r,\n",
    "            lora_alpha=2*r,\n",
    "            lora_dropout=0.0,\n",
    "            bias=(old_linear.bias is not None),\n",
    "            freeze_base=freeze\n",
    "        )\n",
    "        new_linear.weight.data.copy_(old_linear.weight.data)\n",
    "        if old_linear.bias is not None:\n",
    "            new_linear.bias.data.copy_(old_linear.bias.data)\n",
    "        setattr(parent_module, child_name, new_linear)\n",
    "\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"lora_\" in name:\n",
    "                continue\n",
    "            if (task_type == \"seq_cls\" and name.startswith(\"classifier\")) \\\n",
    "               or (task_type == \"causal_lm\" and name.startswith(\"score\")) \\\n",
    "               or (task_type == \"seq2seq\" and name.startswith(\"lm_head\")):\n",
    "                continue\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=======Ranks dict========#\n",
    "\"\"\"\"\n",
    "You can customize the ranks for each layer here.\n",
    "The rank_dict is a dictionary where the keys are the parameter names and the values are the ranks\n",
    "\"\"\"\n",
    "\n",
    "manual_ranks = [\n",
    "    (43, 64),\n",
    "    (9, 59),\n",
    "    (8, 47),\n",
    "    (3, 43),\n",
    "    (4, 50),\n",
    "    (1, 61),\n",
    "    (48, 62),\n",
    "    (46, 60),\n",
    "    (37, 52),\n",
    "    (25, 42),\n",
    "    (10, 49),\n",
    "    (14, 53)\n",
    "]\n",
    "\n",
    "rank_dict = {}\n",
    "for i, (q_rank, v_rank) in enumerate(manual_ranks):\n",
    "    rank_dict[f\"roberta.encoder.layer.{i}.attention.self.query.weight\"] = q_rank\n",
    "    rank_dict[f\"roberta.encoder.layer.{i}.attention.self.value.weight\"] = v_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======Load dataset=======#\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = raw_datasets[\"train\"].shuffle(seed=42).select(range(60000))  # adjust range per GPU\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, \n",
    "                     padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "train_dataset = train_data.map(tokenize_function, batched=True, \n",
    "                               remove_columns=[col for col in train_data.column_names if col != \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======Load base  and train =======#\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "apply_lora_by_names(model, rank_dict, freeze=True, task_type=\"seq_cls\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/himani/SL_SST\",   \n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    optim=\"adamw_hf\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",                  \n",
    "    save_strategy=\"epoch\",                    \n",
    "    evaluation_strategy=\"no\",\n",
    "    max_grad_norm=1.0,  \n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====Evaluation in Benchmarks===#\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt_path = \"/home/himani/SL_SST2/Experiment_8/model.safetensors\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "apply_lora_by_names(model, rank_dict, freeze=True)\n",
    "state_dict = load_file(ckpt_path)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# ======Check if u loaded LoRA properly====== #\n",
    "missing_lora_keys = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name:\n",
    "        if param.data.abs().sum() == 0:\n",
    "            missing_lora_keys.append(name)\n",
    "\n",
    "if missing_lora_keys:\n",
    "    print(\"\\n[WARNING] The following LoRA parameters are all zeros (likely not loaded or not trained):\")\n",
    "    for key in missing_lora_keys:\n",
    "        print(f\" - {key}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] All LoRA parameters appear non-zero and properly loaded.\")\n",
    "\n",
    "# ======Benchmark datasets====== #\n",
    "benchmarks = {\n",
    "    \"sst2\": load_dataset(\"glue\", \"sst2\", split=\"validation\"),\n",
    "    \"imdb\": load_dataset(\"imdb\", split=\"test\")\n",
    "}\n",
    "\n",
    "# ======Preprocessing====== #\n",
    "def preprocess_eval_function(examples, dataset_name):\n",
    "    if dataset_name == \"sst2\":\n",
    "        encoded = tokenizer(\n",
    "            examples[\"sentence\"], truncation=True,\n",
    "            padding=\"max_length\", max_length=128\n",
    "        )\n",
    "    elif dataset_name == \"imdb\":\n",
    "        encoded = tokenizer(\n",
    "            examples[\"text\"], truncation=True,\n",
    "            padding=\"max_length\", max_length=128\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset name.\")\n",
    "    encoded[\"labels\"] = examples[\"label\"]\n",
    "    return encoded\n",
    "\n",
    "# ======Accuracy Calculation====== #\n",
    "def compute_accuracy(dataset):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    for i in range(0, total, 32):\n",
    "        batch = dataset[i:i+32]\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "results = {}\n",
    "in_distribution = []\n",
    "out_distribution = []\n",
    "\n",
    "for name, dataset in benchmarks.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    tokenized = dataset.map(\n",
    "        lambda x: preprocess_eval_function(x, name),\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in dataset.column_names if col not in [\"label\", \"sentence\", \"text\"]]\n",
    "    )\n",
    "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    acc = compute_accuracy(tokenized)\n",
    "    results[name] = acc\n",
    "    if name == \"sst2\":\n",
    "        in_distribution.append(acc)\n",
    "    else:\n",
    "        out_distribution.append(acc)\n",
    "\n",
    "# ======Results ======#\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "print(f\"Avg In-Distribution Accuracy: {np.mean(in_distribution):.4f}\")\n",
    "print(f\"Avg Out-of-Distribution Accuracy: {np.mean(out_distribution):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

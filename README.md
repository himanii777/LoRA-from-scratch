# LoRA-from-scratch
This repository provides a from-scratch implementation of LoRA (Low-Rank Adaptation) applied to the RoBERTa model for efficient fine-tuning on NLP tasks, with SST used as the default benchmark. The core feature of this implementation is the ability to assign different LoRA ranks to each transformer layer, enabling flexible experimentation with layerwise adaptation strategies. The codebase is clean and modular, allowing users to easily swap in custom datasets and evaluation scripts. LoRA is injected directly into the query and value projections of RoBERTaâ€™s self-attention layers, without relying on any external LoRA libraries, ensuring full control and transparency. This setup is ideal for researchers and practitioners looking to explore parameter-efficient fine-tuning in a highly customizable way.
